{"cells":[{"cell_type":"markdown","metadata":{"id":"D58ZoOVP7x2P"},"source":[]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1437,"status":"ok","timestamp":1745383145555,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"0Iwk9OTO8PWu","outputId":"7e8c90c4-870e-41b6-dbdf-55330821fa00"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cpu)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"]}],"source":["# Upgrade to the latest transformers (and optionally accelerate)\n","!pip install --upgrade transformers accelerate"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5812,"status":"ok","timestamp":1745383151375,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"OJsSO97Z79Lg","outputId":"c16ed1cd-379a-4c06-d8d1-4cad6c055df0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","Hit:2 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n","Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","libtesseract-dev is already the newest version (4.1.1-2.1build1).\n","tesseract-ocr is already the newest version (4.1.1-2.1build1).\n","0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n","Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n","Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n"]}],"source":["# Install the Tesseract engine\n","!apt-get update -y\n","!apt-get install -y tesseract-ocr libtesseract-dev\n","\n","# Install the Python wrappers\n","!pip install pytesseract optuna\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1745383151383,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"IV9No3dF78v3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1745383151394,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"JA9PrT7y7x2X"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader, random_split, Dataset\n","\n","# OCR and text model imports\n","from transformers import TrOCRProcessor, VisionEncoderDecoderModel, BertTokenizer, BertModel\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import confusion_matrix, classification_report, f1_score\n","from sklearn.ensemble import GradientBoostingClassifier\n","import pytesseract\n","\n","# Hyperparameter tuning\n","import optuna\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","from PIL import Image, UnidentifiedImageError"]},{"cell_type":"markdown","metadata":{"id":"Byfgvars7x2e"},"source":["#### Configurations"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1745383151403,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"AoeXM_yH7x2k"},"outputs":[],"source":["DATA_DIR = 'receipt_dataset'\n","BATCH_SIZE = 4\n","NUM_EPOCHS = 10\n","LEARNING_RATE = 1e-4\n","VAL_SPLIT = 0.2\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"Su-MWVei7x2k"},"source":["#### Data Augmentation & Transforms"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1745383151413,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"IgdyFBnP7x2k"},"outputs":[],"source":["train_transforms = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","val_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])"]},{"cell_type":"markdown","metadata":{"id":"4sLAix1W7x2o"},"source":["#### Dataset"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1745383151421,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"N_n4JPJh7x2u"},"outputs":[],"source":["class OCRImageTextDataset(Dataset):\n","    def __init__(self, root_dir, image_transform=None, ocr_processor=None, ocr_model=None, text_tokenizer=None, max_length=128):\n","        self.samples = []\n","        self.image_transform = image_transform\n","        self.ocr_processor = ocr_processor\n","        self.ocr_model = ocr_model\n","        self.text_tokenizer = text_tokenizer\n","        self.max_length = max_length\n","\n","        # Get valid class folders\n","        classes = [\n","            d for d in sorted(os.listdir(root_dir))\n","            if os.path.isdir(os.path.join(root_dir, d)) and not d.startswith(\".\")\n","        ]\n","\n","        self.label_encoder = LabelEncoder()\n","        self.label_encoder.fit(classes)\n","\n","        # Collect and verify readable image paths\n","        for cls in classes:\n","            cls_dir = os.path.join(root_dir, cls)\n","            for fname in os.listdir(cls_dir):\n","                img_path = os.path.join(cls_dir, fname)\n","                try:\n","                    with Image.open(img_path) as img:\n","                        img.verify()  # validate image\n","                    self.samples.append((img_path, cls))\n","                except (UnidentifiedImageError, OSError):\n","                    print(f\"⚠️ Skipping corrupted image during init: {img_path}\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        img_path, label = self.samples[idx]\n","\n","        # Load image\n","        try:\n","            image = Image.open(img_path).convert(\"RGB\")\n","        except UnidentifiedImageError:\n","            print(f\"⚠️ Failed to load image at runtime: {img_path}\")\n","            return self.__getitem__((idx + 1) % len(self))  # fallback to next sample\n","\n","        # Transform image\n","        if self.image_transform:\n","            image_tensor = self.image_transform(image)\n","        else:\n","            image_tensor = transforms.ToTensor()(image)\n","\n","        # Extract text using TrOCR or pytesseract fallback\n","        try:\n","            if self.ocr_processor and self.ocr_model:\n","                pixel_values = self.ocr_processor(images=image, return_tensors='pt').pixel_values.to(DEVICE)\n","                generated_ids = self.ocr_model.generate(pixel_values)\n","                text = self.ocr_processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","            else:\n","                text = pytesseract.image_to_string(image)\n","        except Exception as e:\n","            print(f\"⚠️ OCR failed on {img_path}, error: {e}\")\n","            text = \"\"\n","\n","        # Tokenize text\n","        encoding = self.text_tokenizer.encode_plus(\n","            text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","        input_ids = encoding['input_ids'].squeeze(0)\n","        attention_mask = encoding['attention_mask'].squeeze(0)\n","\n","        # Encode label\n","        label_idx = self.label_encoder.transform([label])[0]\n","\n","        return image_tensor, input_ids, attention_mask, label_idx\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1745383151427,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"Duq5wWlDEINv"},"outputs":[],"source":["class HybridClassifier(nn.Module):\n","    def __init__(self, img_feat_dim, txt_feat_dim, num_classes, hidden_dim=256):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(img_feat_dim + txt_feat_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(hidden_dim, num_classes)   # ← now dynamic\n","        )\n","    def forward(self, img_feats, txt_feats):\n","        x = torch.cat([img_feats, txt_feats], dim=1)\n","        return self.fc(x)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":928,"status":"ok","timestamp":1745383152359,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"6tMW8nXH7x2v","outputId":"a5c81ed0-4307-4b60-95c9-98caeb366367"},"outputs":[{"output_type":"stream","name":"stderr","text":["Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"image_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"pooler_act\": \"tanh\",\n","  \"pooler_output_size\": 768,\n","  \"qkv_bias\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.51.3\"\n","}\n","\n","Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_cross_attention\": true,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": 0.0,\n","  \"cross_attention_hidden_size\": 768,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"eos_token_id\": 2,\n","  \"init_std\": 0.02,\n","  \"is_decoder\": true,\n","  \"layernorm_embedding\": true,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"trocr\",\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"use_cache\": false,\n","  \"use_learned_position_embeddings\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["ocr_processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\n","ocr_model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten').to(DEVICE)\n","text_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":879,"status":"ok","timestamp":1745383153242,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"Bm7BU22S7x2v","outputId":"246651c7-dfea-447e-fa19-b6e8bd20a058"},"outputs":[{"output_type":"stream","name":"stdout","text":["⚠️ Skipping corrupted image during init: receipt_dataset/ai_generated/ChatGPT Image Apr 22, 2025, 09_30_14 PM.png\n","⚠️ Skipping corrupted image during init: receipt_dataset/ai_generated/ChatGPT Image Apr 22, 2025, 09_46_52 PM.png\n"]}],"source":["full_dataset = OCRImageTextDataset(DATA_DIR,\n","    image_transform=train_transforms,\n","    ocr_processor=ocr_processor,\n","    ocr_model=ocr_model,\n","    text_tokenizer=text_tokenizer)\n","num_classes = len(full_dataset.label_encoder.classes_)  # e.g. 2\n","\n","hybrid_model = HybridClassifier(\n","    img_feat_dim=1280,\n","    txt_feat_dim=768,\n","    num_classes=num_classes   # ← use this!\n",").to(DEVICE)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1745383153296,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"LZG9BEx5GprG","outputId":"924889ca-e316-4c87-e6e6-ce7f64ff087e"},"outputs":[{"output_type":"stream","name":"stdout","text":["['ai_generated' 'real']\n","2\n"]}],"source":["print(full_dataset.label_encoder.classes_)  # Should be ['ai_generated', 'real']\n","print(len(full_dataset.label_encoder.classes_))  # Should be 2\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1745383153307,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"pKUTVUKa7x21"},"outputs":[],"source":["val_size = int(len(full_dataset) * VAL_SPLIT)\n","train_size = len(full_dataset) - val_size\n","train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1745383153318,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"M7N36uui7x21"},"outputs":[],"source":["# apply val transforms\n","train_dataset.dataset.image_transform = train_transforms\n","val_dataset.dataset.image_transform = val_transforms"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1745383153328,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"ES43NcQT7x22"},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"VQtO9CR07x22"},"source":["#### Feature Extractors"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":304,"status":"ok","timestamp":1745383153635,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"0c4yRRQS7x28","outputId":"ef057f3b-f949-4a26-8929-7eaa01d26bca"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n","100%|██████████| 20.5M/20.5M [00:00<00:00, 139MB/s] \n"]}],"source":["# Image model\n","image_model = models.efficientnet_b0(pretrained=True)\n","image_model.classifier = nn.Identity()  # remove classification head\n","image_model = image_model.to(DEVICE)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":2258,"status":"ok","timestamp":1745383155877,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"f8FLf7eK7x29","colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["62cfe39624f4448aa57cfdafc3895cf1","ca07d5ad360e4f3a9076292fe364216c","101cc1b49c1441d0998c18a5e7b35737","ef4879a298c34c2e8e42ec3030583661","efba2e8bf21d4c1584ef2eadd759110f","428a83724e084e699cfa933f3ecfd591","0b535b59a18e407fa03f859a2722363c","226215b2c6ce4475b6e1a46bb01bd4ea","60038084c2744f09a81e3800d4daa676","a3c90eeed3f8461b883836291230151d","72d593e735fd40d684e16f8e280888ae"]},"outputId":"1402c1ed-2ac6-4255-b3a5-3a7dc9cf74b4"},"outputs":[{"output_type":"stream","name":"stderr","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62cfe39624f4448aa57cfdafc3895cf1"}},"metadata":{}}],"source":["# Text model\n","text_model = BertModel.from_pretrained('bert-base-uncased').to(DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"3R5zW00B7x29"},"source":["####Combine Features & Classifier"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":108,"status":"ok","timestamp":1745383155988,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"3z9PIXwr7x3D"},"outputs":[],"source":["hybrid_model = HybridClassifier(img_feat_dim=1280, txt_feat_dim=768, num_classes=2).to(DEVICE)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(list(image_model.parameters()) +\n","                       list(text_model.parameters()) +\n","                       list(hybrid_model.parameters()), lr=LEARNING_RATE)"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1745383155993,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"odR64C4s7x3D"},"outputs":[],"source":["# LR Scheduler & Early Stopping\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n","early_stop_patience = 3"]},{"cell_type":"markdown","metadata":{"id":"fpCD6MWS7x3E"},"source":["####Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SmpxqUuE7x3E","outputId":"7fc5c227-667a-4b42-8a08-3db34fa5f723"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 — Train Loss: 0.5211, Accuracy: 0.7578\n","   Val Loss: 0.1987, Val Accuracy: 0.9750\n"]}],"source":["for epoch in range(1, NUM_EPOCHS + 1):\n","    hybrid_model.train(); image_model.train(); text_model.train()\n","    total_loss = correct = 0\n","\n","    for imgs, ids, masks, labels in train_loader:\n","        imgs, ids, masks, labels = [x.to(DEVICE) for x in (imgs, ids, masks, labels)]\n","\n","        optimizer.zero_grad()\n","        img_feats = image_model(imgs)\n","        txt_out = text_model(input_ids=ids, attention_mask=masks, output_hidden_states=True)\n","        txt_feats = txt_out.logits if hasattr(txt_out, 'logits') else txt_out.last_hidden_state[:, 0, :]\n","\n","        logits = hybrid_model(img_feats, txt_feats)\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * labels.size(0)\n","        correct += (logits.argmax(1) == labels).sum().item()\n","\n","    avg_loss = total_loss / len(train_loader.dataset)\n","    acc = correct / len(train_loader.dataset)\n","    print(f\"Epoch {epoch} — Train Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}\")\n","\n","    # ---- Validation ----\n","    hybrid_model.eval(); image_model.eval(); text_model.eval()\n","    val_loss = correct = 0\n","    all_preds, all_labels = [], []\n","    with torch.no_grad():\n","        for imgs, ids, masks, labels in val_loader:\n","            imgs, ids, masks, labels = [x.to(DEVICE) for x in (imgs, ids, masks, labels)]\n","            img_feats = image_model(imgs)\n","            txt_out = text_model(input_ids=ids, attention_mask=masks, output_hidden_states=True)\n","            txt_feats = txt_out.logits if hasattr(txt_out, 'logits') else txt_out.last_hidden_state[:, 0, :]\n","\n","            logits = hybrid_model(img_feats, txt_feats)\n","            loss = criterion(logits, labels)\n","            val_loss += loss.item() * labels.size(0)\n","            correct += (logits.argmax(1) == labels).sum().item()\n","\n","            all_preds.extend(logits.argmax(1).cpu().tolist())\n","            all_labels.extend(labels.cpu().tolist())\n","\n","    avg_val_loss = val_loss / len(val_loader.dataset)\n","    val_acc = correct / len(val_loader.dataset)\n","    scheduler.step(avg_val_loss)\n","\n","    print(f\"   Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n","\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        torch.save({\n","            'image_model': image_model.state_dict(),\n","            'text_model': text_model.state_dict(),\n","            'hybrid_model': hybrid_model.state_dict()\n","        }, 'best_hybrid_model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":129,"status":"aborted","timestamp":1745383532350,"user":{"displayName":"Muhammed Yuguda","userId":"12316331719822797645"},"user_tz":-60},"id":"8Hnj9k-p7x3F"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["#### Evaluation"],"metadata":{"id":"YnlnFwgA9C5M"}},{"cell_type":"code","source":["cm = confusion_matrix(all_labels, all_preds)\n","print(\"Confusion Matrix:\\n\", cm)\n","print(\"Classification Report:\\n\", classification_report(all_labels, all_preds))\n","print(\"F1 Score:\", f1_score(all_labels, all_preds, average='weighted'))"],"metadata":{"id":"vpTBsUGp8yia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(cm, cmap='Blues')\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.colorbar()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"DBEeoNjC_NY6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Optuna"],"metadata":{"id":"Zs73CJ7t_VmS"}},{"cell_type":"code","source":["def objective(trial):\n","    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n","    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n","    return best_val_loss  # Placeholder — insert training loop here"],"metadata":{"id":"PSMH_xQF_O-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# study = optuna.create_study(direction='minimize')\n","# study.optimize(objective, n_trials=20)\n","# print('Best Params:', study.best_params)"],"metadata":{"id":"sobp4jbb_bCN"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"62cfe39624f4448aa57cfdafc3895cf1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ca07d5ad360e4f3a9076292fe364216c","IPY_MODEL_101cc1b49c1441d0998c18a5e7b35737","IPY_MODEL_ef4879a298c34c2e8e42ec3030583661"],"layout":"IPY_MODEL_efba2e8bf21d4c1584ef2eadd759110f"}},"ca07d5ad360e4f3a9076292fe364216c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_428a83724e084e699cfa933f3ecfd591","placeholder":"​","style":"IPY_MODEL_0b535b59a18e407fa03f859a2722363c","value":"model.safetensors: 100%"}},"101cc1b49c1441d0998c18a5e7b35737":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_226215b2c6ce4475b6e1a46bb01bd4ea","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60038084c2744f09a81e3800d4daa676","value":440449768}},"ef4879a298c34c2e8e42ec3030583661":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3c90eeed3f8461b883836291230151d","placeholder":"​","style":"IPY_MODEL_72d593e735fd40d684e16f8e280888ae","value":" 440M/440M [00:01&lt;00:00, 243MB/s]"}},"efba2e8bf21d4c1584ef2eadd759110f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"428a83724e084e699cfa933f3ecfd591":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b535b59a18e407fa03f859a2722363c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"226215b2c6ce4475b6e1a46bb01bd4ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60038084c2744f09a81e3800d4daa676":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a3c90eeed3f8461b883836291230151d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72d593e735fd40d684e16f8e280888ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}